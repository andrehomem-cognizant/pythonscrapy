2b7246de	11/06/2025 00:00:00	45051	Bulgaria	ShaneLei.Abelende@cognizant.com	Onboarding Qualify	11/6/25, 4:52 PM	11/6/25, 5:01 PM		11/6/25	Completed										

12:19:26 PM	Notice	Execution started
12:19:27 PM	Info	--- Starting Audit for Case ID: 45051 ---
12:19:27 PM	Info	Step 1: Fetching from AppSheet "Completed_Cases" table...
12:19:46 PM	Info	 -> Found 0 completed task(s) in AppSheet for this case.
12:19:46 PM	Info	
Step 2: Fetching from Google Sheet "Active Cases"...
12:19:46 PM	Info	 -> Found 1 active task(s) in Google Sheets for this case.
12:19:46 PM	Info	    - Task Type: Asset Delivery, Log ID: 34acc136
12:19:46 PM	Info	
Step 3: Reading from the destination Sub_Tasks.csv...
12:19:49 PM	Info	 -> Found 1 task(s) in the final CSV for this case.
12:19:49 PM	Info	    - Task Type: Asset Delivery, Log ID: 34acc136
12:19:49 PM	Info	
Step 4: Analysis...
12:19:49 PM	Info	--- AUDIT FINDINGS: ALL RECORDS ACCOUNTED FOR ---
12:19:49 PM	Info	All tasks for this case seem to be correctly represented in the final CSV file.
12:19:49 PM	Info	--- Audit for Case ID: 45051 Complete ---
12:19:48 PM	Notice	Execution completed




it should be counting cases but if it's filtered based on the task type it will only count the case IDs under those task types. 
 
 
total handled handled under OBQ Task Type - the number of unique case IDs within the date range that has OBQ Task type
Completed cases - case IDs under OBQ Task type with end timestamp wihtin the date range
Inprogress cases - case IDs under OBQ Task type without end timestamp wihtin the date range or end timestamps after the end date
 
Does this make more sense?
 



Hello, I need to adapt my existing Node.js Playwright scraper (hosted on Google Cloud Functions) to perform structured data extraction from restaurant menu pages like Bolt Food.

The goal is to extract a complete, organized list of all menu items and their options, not just the raw HTML.

The scraper must be built to parse the page and find these specific data points for every dish:

Category (e.g., "Burgers", "Drinks")

Dish Name

Price

Description

Image URL

Option Groups (e.g., "Choose Your Size", "Add-ons")

Options (e.g., "Small: $1.00", "Large: $2.00", "Bacon: $1.50")

Here is the required workflow for the Cloud Function:

Scrape and Parse: It must launch Playwright, navigate to the URL, and loop through the page structure to extract all the data points listed above into a structured JSON array.

Format Data: It must convert this final JSON array into a clean CSV-formatted string.

Hash the Content: It must calculate a SHA256 hash of the final CSV string.

Save to Storage: It must save the CSV string as a new file in a Google Cloud Storage bucket. The filename must be the hash (e.g., 5d1a89b....csv).

Generate Link: It must generate a Signed v4 URL for that new file, setting it to expire in 15 minutes.

Return JSON: The function should return a JSON object to my calling app containing the hash, the dishCount, and the temporary signedUrl for the user to download.

Finally, please also provide instructions for setting a Lifecycle Rule on the Cloud Storage bucket to automatically delete all files after 1 day to ensure we do not incur long-term storage costs."
